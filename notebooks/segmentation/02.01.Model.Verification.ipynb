{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage import io\n",
    "from skimage import exposure\n",
    "\n",
    "import os\n",
    "import gzip\n",
    "import gc\n",
    "import pprint as pp\n",
    "import numpy as np\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import logging\n",
    "import random\n",
    "from datetime import datetime\n",
    "\n",
    "import keras\n",
    "from keras.models import load_model\n",
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_location = \"../../logs/\"\n",
    "PATH_TO_INTERMEDIATE = \"../../data/segmentation.pickle\"\n",
    "PATH_TO_MODELS = \"../../data/segmentation.models\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger()\n",
    "logging.basicConfig(format=\"%(asctime)-15s %(message)s\",\n",
    "                    level=logging.DEBUG,\n",
    "                    filename=os.path.join(log_location,'validation.' + datetime.now().strftime(\"%Y%m%d%H%M%S.%f\") + '.log'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel_size = 48\n",
    "half_kernel = int(kernel_size/2)\n",
    "num_classes = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_data = []\n",
    "for current_model in os.listdir(PATH_TO_INTERMEDIATE):\n",
    "    if not (current_model.endswith('x.pickle') and current_model.startswith('validation')):\n",
    "        continue\n",
    "    validation_data.append(os.path.join(PATH_TO_INTERMEDIATE,current_model))  \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log(msg):\n",
    "    logging.debug(msg)\n",
    " \n",
    "def print_log(msg):\n",
    "    log(msg)\n",
    "    print(msg)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_config = {}\n",
    "model_evaluation_file_name = os.path.join(PATH_TO_MODELS,'model.evaluation.pickle')\n",
    "model_evaluation = {}\n",
    "\n",
    "for current_model in os.listdir(PATH_TO_MODELS):\n",
    "    if not (current_model.endswith(\".pickle\") and current_model.startswith(\"2\")):\n",
    "        continue\n",
    "    current_model = os.path.join(PATH_TO_MODELS,current_model)\n",
    "    with gzip.open(current_model,'rb') as f:\n",
    "            model_config = pickle.load(f)  \n",
    "\n",
    "    model = load_model(model_config['model_file_name'])\n",
    "    for current_data_set in validation_data:\n",
    "        with gzip.open(current_data_set,'rb') as f:\n",
    "            validation_x = pickle.load(f)\n",
    "        with gzip.open(current_data_set.replace('.x.pickle','.y.pickle'),'rb') as f:\n",
    "            validation_y = pickle.load(f)\n",
    "\n",
    "        validation_x = (validation_x / 128) - 1\n",
    "        validation_x = validation_x.reshape(validation_x.shape[0], kernel_size, kernel_size,1)\n",
    "        validation_y = validation_y.reshape(validation_y.shape[0])\n",
    "        validation_y = keras.utils.to_categorical(validation_y, num_classes)\n",
    "            \n",
    "        current_data_set_name = current_data_set.split('/')[-1]\n",
    "        model_name = model_config['model_file_name'].split('/')[-1]\n",
    "        print_log(model_name)\n",
    "        print_log(current_data_set_name)\n",
    "        model_evaluation[model_name]= {}\n",
    "        model_evaluation[model_name]['name']=model_name\n",
    "        model_evaluation[model_name]['scores'] = {}\n",
    "        \n",
    "        score = model.evaluate(validation_x, validation_y, verbose=1)\n",
    "        model_evaluation[model_name]['scores'][current_data_set_name] = score\n",
    "        print_log(score)\n",
    "        print_log(model_evaluation)\n",
    "        with gzip.open(model_evaluation_file_name,'wb') as f:\n",
    "            pickle.dump(model_evaluation, f, protocol=pickle.HIGHEST_PROTOCOL)  \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
