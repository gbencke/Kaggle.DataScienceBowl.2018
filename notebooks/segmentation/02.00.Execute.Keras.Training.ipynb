{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "import h5py\n",
    "from keras.datasets import mnist\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D, Activation\n",
    "from keras import backend as K\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "from random import shuffle\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "from os.path import isfile, join\n",
    "from os import listdir\n",
    "from datetime import datetime\n",
    "\n",
    "import logging\n",
    "import math\n",
    "import pickle\n",
    "import gc\n",
    "from random import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_location = \"../../logs/\"\n",
    "PATH_TO_INTERMEDIATE = \"../../data/segmentation.pickle/0\"\n",
    "PATH_TO_MODELS = \"../../data/segmentation.models/0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger()\n",
    "logging.basicConfig(format=\"%(asctime)-15s %(message)s\",\n",
    "                    level=logging.DEBUG,\n",
    "                    filename=os.path.join(log_location,'keras.' + datetime.now().strftime(\"%Y%m%d%H%M%S.%f\") + '.log'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def IsDevelopmentEnvironment():\n",
    "    return False\n",
    "\n",
    "DevelopmentEnvironment = IsDevelopmentEnvironment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log(msg):\n",
    "    logging.debug(msg)\n",
    " \n",
    "def print_log(msg):\n",
    "    log(msg)\n",
    "    print(msg)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_batches_x = sorted([join(PATH_TO_INTERMEDIATE, x) for x in listdir(PATH_TO_INTERMEDIATE) if isfile(join(PATH_TO_INTERMEDIATE, x)) and x.endswith(\"x.pickle\")  and x.startswith('train')])\n",
    "validation_batches_x = sorted([join(PATH_TO_INTERMEDIATE, x) for x in listdir(PATH_TO_INTERMEDIATE) if isfile(join(PATH_TO_INTERMEDIATE, x)) and x.endswith(\"x.pickle\") and x.startswith('validation')])\n",
    "\n",
    "print_log(\"Original number of train batches:{}\".format(len(train_batches_x)))\n",
    "print_log(\"Original number of validation batches:{}\".format(len(validation_batches_x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = None\n",
    "train_y = None\n",
    "\n",
    "validation_x = None\n",
    "validation_y = None\n",
    "\n",
    "test_x = None\n",
    "test_y = None\n",
    "\n",
    "current_train_batch = 0\n",
    "current_validation_batch = 0\n",
    "current_test_batch = 0\n",
    "\n",
    "print_log(\"Finished Resetting the global arrays...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_train_batch_file_name_x = join(PATH_TO_INTERMEDIATE, \"train.batch.{}.x.pickle\".format(current_train_batch))\n",
    "current_train_batch_file_name_y = join(PATH_TO_INTERMEDIATE, \"train.batch.{}.y.pickle\".format(current_train_batch))\n",
    "with open(current_train_batch_file_name_x,'rb') as f:\n",
    "    train_x = pickle.load(f)\n",
    "with open(current_train_batch_file_name_y,'rb') as f:\n",
    "    train_y = pickle.load(f) \n",
    "\n",
    "batch_total_number_samples = train_x.shape[0]\n",
    "\n",
    "print_log(\"Finished Reading the sample train batches...\")    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "num_classes = 2\n",
    "epochs = 1\n",
    "development_machine_samples = 10000\n",
    "img_rows, img_cols = train_x.shape[1], train_x.shape[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id = 0\n",
    "training_parameters = []\n",
    "for current_num_models in range(1):\n",
    "    for current_filters_number in [32,24]:\n",
    "        for current_dropout in [0.50]:\n",
    "            training_parameters.append(\n",
    "                { 'id' : id, \n",
    "                  'interaction': current_num_models, \n",
    "                  'filters' : current_filters_number, \n",
    "                  'dropout' : current_dropout })\n",
    "            id = id + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first = True\n",
    "\n",
    "total_validation_x = None\n",
    "total_validation_y = None\n",
    "\n",
    "validation_x = None\n",
    "validation_y = None\n",
    "\n",
    "for current_validation in validation_batches_x[:1]:\n",
    "    print_log(\"Merging Validation Batch:{}\".format(current_validation))\n",
    "    with open(current_validation,'rb') as f:\n",
    "        validation_x = pickle.load(f)\n",
    "    with open(current_validation.replace(\"x.pickle\",\"y.pickle\"),'rb') as f:\n",
    "        validation_y = pickle.load(f) \n",
    "    if not first:\n",
    "        total_validation_x = np.concatenate((total_validation_x, validation_x))\n",
    "        total_validation_y = np.concatenate((total_validation_y, validation_y))\n",
    "    else:\n",
    "        total_validation_x = validation_x\n",
    "        total_validation_y = validation_y\n",
    "    first = False\n",
    "    \n",
    "validation_x = total_validation_x  \n",
    "validation_y = total_validation_y\n",
    "\n",
    "print_log(\"Finished merging validation batches...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "general_input_shape = (img_rows, img_cols, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "general_input_shape\n",
    "batch_total_number_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(parameters):\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(parameters['filters'], kernel_size=(2, 2),\n",
    "                     activation='relu',\n",
    "                     input_shape=general_input_shape))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(parameters['dropout']))\n",
    "\n",
    "    model.add(Conv2D(parameters['filters'], kernel_size=(2, 2),\n",
    "                     activation='relu',\n",
    "                     input_shape=general_input_shape))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(parameters['dropout']))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(parameters['filters'], activation='relu'))\n",
    "    model.add(Dropout(parameters['dropout']))\n",
    "\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=keras.optimizers.Adadelta(),\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DevelopmentEnvironment = False\n",
    "print_log(\"Global epochs:{}\".format(len(training_parameters)))\n",
    "first = True\n",
    "for current_parameter in training_parameters:\n",
    "    print(str(current_parameter))\n",
    "    train_x = None\n",
    "    train_y = None\n",
    "    gc.collect()\n",
    "    model = create_model(current_parameter)\n",
    "    histories = []\n",
    "    \n",
    "    num_steps_per_batch = 100\n",
    "    original_sample_size = batch_total_number_samples\n",
    "    current_sample_size_for_fit = int(original_sample_size / (current_parameter['filters'] * 12))\n",
    "    number_of_interactions = int(original_sample_size / current_sample_size_for_fit)\n",
    "\n",
    "    for current_interaction in range(number_of_interactions):\n",
    "        for current_train in train_batches_x:\n",
    "            i = datetime.now()\n",
    "            current_date_time = i.strftime('%Y%m%d.%H%M%S')\n",
    "            \n",
    "            print_log(\"Loading for Training Batch:{} on GlobalEpoch:{}\".format(current_train, current_parameter['id']))\n",
    "            with open(current_train,'rb') as f:\n",
    "                train_x = pickle.load(f)\n",
    "            with open(current_train.replace(\"x.pickle\",\"y.pickle\"),'rb') as f:\n",
    "                train_y = pickle.load(f) \n",
    "\n",
    "            print_log(\"Sampling...\")\n",
    "            \n",
    "            start_sample = current_sample_size_for_fit * current_interaction\n",
    "            end_sample = current_sample_size_for_fit * (current_interaction+1)\n",
    "\n",
    "            print_log(\"Iniciando o fit ({} in {}, now running on {} to {})...\".format(original_sample_size, number_of_interactions, start_sample,end_sample))\n",
    "            \n",
    "            train_x = train_x[start_sample:end_sample]\n",
    "            train_y = train_y[start_sample:end_sample]\n",
    "\n",
    "            current_history = model.fit(train_x, train_y,\n",
    "                      epochs = int(train_x.shape[0] / num_steps_per_batch),\n",
    "                      verbose=1,\n",
    "                      steps_per_epoch=num_steps_per_batch,\n",
    "                      callbacks=[\n",
    "                          EarlyStopping(\n",
    "                          monitor='loss',\n",
    "                          min_delta=0.001,                              \n",
    "                          patience=2,                              \n",
    "                          verbose=1,\n",
    "                          mode='auto')])\n",
    "            histories.append(current_history)\n",
    "            current_interaction = current_interaction + 1\n",
    "    \n",
    "    score = model.evaluate(validation_x, validation_y, verbose=1)\n",
    "    batch_report = { 'batch_id' :  current_parameter['id'], 'parameters' : current_parameter } \n",
    "    print_log('Test loss:{}'.format(score[0]))\n",
    "    print_log('Test accuracy:{}'.format(score[1]))\n",
    "    batch_report['score'] = score\n",
    "    #batch_report['history'] = histories\n",
    "    batch_report['input_shape'] = general_input_shape\n",
    "\n",
    "    model_file_name = join(PATH_TO_MODELS,'{}.model.meta.b{}.{}.h5'.format(current_date_time, current_parameter['id'],score[1]))\n",
    "    batch_report['model_file_name'] = model_file_name\n",
    "    with open(model_file_name.replace(\".h5\",\".pickle\"),'wb') as f:\n",
    "        pickle.dump(batch_report, f, protocol=pickle.HIGHEST_PROTOCOL)  \n",
    "    model.save(model_file_name)\n",
    "    print_log('Model saved to:{}'.format(model_file_name))\n",
    "    print_log('Model metadata saved to:{}'.format(model_file_name.replace(\".h5\",\".pickle\")))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
