{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple MNist like model for detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using as example:\n",
    "https://github.com/keras-team/keras/blob/master/examples/mnist_cnn.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras import backend as K\n",
    "\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from pathlib import Path\n",
    "\n",
    "import gzip\n",
    "import pickle\n",
    "import gc\n",
    "from random import shuffle\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "PATH_TO_INTERMEDIATE = \"../data/intermediate\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in Path(PATH_TO_INTERMEDIATE).glob(\"train*.pickle\"):\n",
    "    p.unlink()\n",
    "for p in Path(PATH_TO_INTERMEDIATE).glob(\"test*.pickle\"):\n",
    "    p.unlink()\n",
    "for p in Path(PATH_TO_INTERMEDIATE).glob(\"validation*.pickle\"):\n",
    "    p.unlink()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "onlyfiles = [join(PATH_TO_INTERMEDIATE, x) for x in listdir(PATH_TO_INTERMEDIATE) if isfile(join(PATH_TO_INTERMEDIATE, x)) and x.endswith(\".pickle\")]\n",
    "shuffle(onlyfiles)\n",
    "print(\"Original number of files:\", len(onlyfiles))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_files_size = 570\n",
    "validation_files_size = 50\n",
    "test_files_size = 50\n",
    "batch_size = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_files_to_use = train_files_size + validation_files_size + test_files_size\n",
    "number_of_files_available = len(onlyfiles)\n",
    "\n",
    "if (number_of_files_to_use) > number_of_files_available:\n",
    "    raise ValueError(\"Total files to use {} is greater than the number of available files {}  \".format(number_of_files_to_use,number_of_files_available))\n",
    "    \n",
    "if (train_files_size % batch_size != 0):\n",
    "    raise ValueError(\"The number of training files need to be a multiple of the batch size\")\n",
    "if (validation_files_size % batch_size != 0):\n",
    "    raise ValueError(\"The number of validation files need to be a multiple of the batch size\")\n",
    "if (test_files_size % batch_size != 0):\n",
    "    raise ValueError(\"The number of test files need to be a multiple of the batch size\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_files = onlyfiles[0:train_files_size]\n",
    "validation_files = onlyfiles[train_files_size:train_files_size + validation_files_size]\n",
    "test_files = onlyfiles[train_files_size + validation_files_size:train_files_size + validation_files_size + test_files_size]\n",
    "\n",
    "print('length of train_files:', len(train_files))\n",
    "print('length of validation_files:', len(validation_files))\n",
    "print('length of test_files:', len(test_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "for current_batch in range(int(train_files_size/batch_size)):\n",
    "    train_x = []\n",
    "    train_y = []\n",
    "    for current_file in train_files[current_batch * batch_size : (current_batch + 1)  * batch_size]:\n",
    "        sample = []\n",
    "        with gzip.open(current_file,'rb') as f:\n",
    "            sample = pickle.load(f)\n",
    "        for current_sample in sample['slices']:\n",
    "            train_x.append(current_sample['slice'])\n",
    "            train_y.append(current_sample['is_nuclei'])\n",
    "    train_x = np.array(train_x)\n",
    "    train_x = train_x.astype('float16')\n",
    "    train_y = np.array(train_y)\n",
    "    train_y = train_y.astype('float16')\n",
    "    with gzip.open(join(PATH_TO_INTERMEDIATE, 'train.batch.{}.x.pickle'.format(current_batch)),'wb') as f:\n",
    "        pickle.dump(train_x, f, protocol=pickle.HIGHEST_PROTOCOL)  \n",
    "    with gzip.open(join(PATH_TO_INTERMEDIATE, 'train.batch.{}.y.pickle'.format(current_batch)),'wb') as f:\n",
    "        pickle.dump(train_y, f, protocol=pickle.HIGHEST_PROTOCOL)  \n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "for current_batch in range(int(validation_files_size/batch_size)):\n",
    "    validation_x = []\n",
    "    validation_y = []\n",
    "    for current_file in validation_files[current_batch * batch_size : (current_batch + 1)  * batch_size]:\n",
    "        sample = []\n",
    "        with gzip.open(current_file,'rb') as f:\n",
    "            sample = pickle.load(f)\n",
    "        for current_sample in sample['slices']:\n",
    "            validation_x.append(current_sample['slice'])\n",
    "            validation_y.append(current_sample['is_nuclei'])\n",
    "    validation_x = np.array(validation_x)\n",
    "    validation_x = validation_x.astype('float16')\n",
    "    validation_y = np.array(validation_y)\n",
    "    validation_y = validation_y.astype('float16')\n",
    "    with gzip.open(join(PATH_TO_INTERMEDIATE, 'validation.batch.{}.x.pickle'.format(current_batch)),'wb') as f:\n",
    "        pickle.dump(validation_x, f, protocol=pickle.HIGHEST_PROTOCOL)  \n",
    "    with gzip.open(join(PATH_TO_INTERMEDIATE, 'validation.batch.{}.y.pickle'.format(current_batch)),'wb') as f:\n",
    "        pickle.dump(validation_y, f, protocol=pickle.HIGHEST_PROTOCOL)  \n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "for current_batch in range(int(test_files_size/batch_size)):\n",
    "    test_x = []\n",
    "    test_y = []\n",
    "    for current_file in test_files[current_batch * batch_size : (current_batch + 1)  * batch_size]:\n",
    "        sample = []\n",
    "        with gzip.open(current_file,'rb') as f:\n",
    "            sample = pickle.load(f)\n",
    "        for current_sample in sample['slices']:\n",
    "            test_x.append(current_sample['slice'])\n",
    "            test_y.append(current_sample['is_nuclei'])\n",
    "    test_x = np.array(test_x)\n",
    "    test_x = test_x.astype('float16')\n",
    "    test_y = np.array(test_y)\n",
    "    test_y = test_y.astype('float16')\n",
    "    with gzip.open(join(PATH_TO_INTERMEDIATE, 'test.batch.{}.x.pickle'.format(current_batch)),'wb') as f:\n",
    "        pickle.dump(test_x, f, protocol=pickle.HIGHEST_PROTOCOL)  \n",
    "    with gzip.open(join(PATH_TO_INTERMEDIATE, 'test.batch.{}.y.pickle'.format(current_batch)),'wb') as f:\n",
    "        pickle.dump(test_y, f, protocol=pickle.HIGHEST_PROTOCOL)  \n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
